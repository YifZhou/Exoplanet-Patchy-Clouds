\documentclass[apj]{emulateapj}

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{natbib}
\bibliographystyle{apj}
\usepackage[breaklinks,colorlinks,citecolor=blue,linkcolor=magenta]{hyperref} 
\shorttitle{Short Title}
\shortauthors{Zhou et al.}

%%% new command %%%
\newcommand{\ima}{\texttt{ima} file}
\newcommand{\flt}{\texttt{flt} file}
\newcommand{\eps}{$\mathrm{e}^{-}/\mathrm{s}$}
\begin{document}
\title{Cloud structure of planetary mass companion AB Pic-b}
\author{Yifan Zhou, Daniel Apai, ...}
\affil{University of Arizona}

\begin{abstract}
...
\end{abstract}

\keywords{kw1, kw2, ...}
\maketitle
%
\section{Introcduction}

\section{Observation}
\section{Data Reduction}

\subsection{Cosmic Ray Identification}
The WFC3 \texttt{calwf3} pipeline produces two types of calibrated
files per exposure, the \ima{} and the \flt.  The \ima{} contains all
calibrated non-destructive readouts. The \flt{} is additionally
processed with an up-the-ramp fit that combines every readout into a
single image with a linear regression for every pixel. In the
meanwhile, up-the-ramp fit identifies cosmic ray by searching for
outliers from the fitting result and makes correction
coordinately. \citeauthor{Mandell2013} suggested that WFC3 IR time series
extract from {\flt}s have a rms 1.3 times larger than that obtained
from {\ima}s with their transit spectroscopic data analysis. Similar result was found in our analysis of WFC3 IR
images. We speculate the larger scattering measured from {\flt}s is
caused by jitter of the telescope, especially when number of sample
per exposure is small. \citeauthor{Swain2013} also recommended to use
non-destructive readouts in data reduction.  

We use the last readout in which the peak pixel of AB Pic b's image is
not saturated for every exposure to carry out photometry
measurement. For cosmic ray identification, we take a linear fit to
the sums of pixels in a $7\times7$ box centered on the peak pixel for
all readouts that do not have unsaturated pixel in that region. We
identify exposures that have reduced $\chi^{2}$ values greater than
2.5 as being contaminated by cosmic ray Using this method, there are 9
out 348 exposures that cosmic ray hits were found in the AB Pic b's
image region with this criterion. We did not make correction for these
exposures considering the small number of readouts. Instead we exclude
these images in further analysis. The scatterings of photometry
measurements using files processed in this method is reduced by
factors of 1.65 and 1.41 for F125W data set and F160W data set
coordinately comparing to those measured with {\flt}s, which is
consistent with \citet{Mandell2013}.

For 2M1207 system, we decided to use the \flt. The image of 2M1207 A
quickly saturated after several non-destructive readouts. The \flt{}
keeps non-saturated file, making precise photometry for both 2M1207 A
and B possible. 

\subsection{Continuous Bad Pixels Identification}
Pixels with data quality flags 'bad detector pixels' (DQ = 4),
'unstable response' (DQ = 32), and 'bad or uncertain flat value' (DQ =
512) were masked out as suggested by previous exoplanet transit
spectroscopic observations\citep[e.g.][]{Berta2012, Kreidberg2014}.

\subsection{Flat Field Correction}
\subsection{AB Pic B: Primary Star PSF Subtraction}
We removed the point spread function (PSF) of the primary star with
standard roll subtraction method. For one target image, every other
image that was taken with the same filter and different roll angle of
the telescope was selected to form a PSF image cube. The PSF of
primary star was subtracted by its own PSF in the PSF image.  The
position offset of the target image and the PSF image was calculated
with cross correlation. The PSF image was shifted using bilinear
interpolation to align with the target image. By optimizing with least
rms residual in the shaded region shown in Figure \ref{fig:image}, the
amplitude scale of the PSF image was calculated. The PSF image
provided the smallest subtraction residual among images in the PSF
cube was selected as final PSF to be subtracted from target image.

Using images with primary star subtracted with above method, an
average background residual is reached as 2.1 \eps, which is more than
one order of magnitude smaller than the photon noise. Therefore, our
photometry is photon noise limited and we did not attempt to use
more sophisticated PSF subtraction strategy.


\begin{figure}
  \centering
  \plotone{original}
  \plotone{subtracted}
  \caption{One example of the original(upper panel) and primary star
    subtracted (lower panel) images of AB Pic system. In the lower
    panel, white contour defines a region of an annulus with circle
    excluded. The rms residuals were optimized in this region when
    calculation the amplitude scales of PSF images when doing primary
    star image subtraction.}
  \label{fig:image}
\end{figure}
\subsection{AB Pic B: Aperture Photometry}

We performed aperture photometry using images processed with above
procedures.  Centroids of PSFs of AB Pic b were located by 2 dimensional
Gaussian fit with IDL routine \texttt{mpfit2dpeak}
\citep{Markwardt2009}. Then aperture photometry was calculated using
\texttt{aper} with aperture radius of 3 pixels. Aperture size is
determined by taking into account both minimizing noise level as well as
excluding bad pixels.

Photometry uncertainty is the combination of three independent components, readout
noise, photon noise, and fluctuation in the background. We take the
square root of the sum of the squares of the three components as the
uncertainty of one photometry measurement. In fact, the uncertainty is
denominated by photon noise.

\subsection{2M1207: Multi-component PSF photometry}

The small angular separation of 2M1207 A and B makes precise primary
star subtraction and photometry very difficult. On the under-sampled
WFC3 IR detector, the primary and the secondary only separate by
$\sim 6$ pixels, which is about 5 times of the FWHM of the PSF. In
addition, under-sampling of the PSF causes significant artifacts when
shifting the PSF to align with image to be subtracted no matter what
interpolation method is used.

To over come above problems, we used several approach to reach high
precision photometry in this extreme case. Our first approach makes
use of  Tiny Tim PSF simulator. Tiny Tim can produce model PSF based on
the filter, spectrum of target, focus status, and the telescope
jitter. One advantage of Tiny Tim PSF over observed PSF is that Tiny Tim
can produce over-sampled PSF, which makes the shifting and
interpolation rather straight forward. However, Tiny Tim has systematic
errors in producing model PSFs. E.G. the diffraction spikes and coma
are not well simulated in Tiny Tim PSFs \citep{Biretta2014}.

We first produced a list of 10x oversampled PSFs based on the positions and spectrum
of 2M1207A with different telescope jitter ranging from 0 to 50
mini-arcsec per second in both $x$ and $y$ direction. The PSFs were
aligned with the original image by minimizing the residual of a region
centered on 2M1207 A with the image of 2M1207 B mostly excluded. 


\section{Result}
\section{Discussion}
\bibliography{ref.bib}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
